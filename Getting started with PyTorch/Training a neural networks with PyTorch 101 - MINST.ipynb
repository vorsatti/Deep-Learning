{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "# Flatten the input images\n",
    "#images.shape[0] is the batch size that is equal to 64\n",
    "#-1 is equivalent of 784\n",
    "inputs = images.view(images.shape[0], -1)\n",
    "\n",
    "# Create parameters\n",
    "w1 = torch.randn(784, 256)\n",
    "b1 = torch.randn(256)\n",
    "\n",
    "w2 = torch.randn(256, 10)\n",
    "b2 = torch.randn(10)\n",
    "\n",
    "#from input to hidden layer\n",
    "h = activation(torch.mm(inputs, w1) + b1)\n",
    "\n",
    "#from hidden layer to output\n",
    "out = torch.mm(h, w2) + b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to pass in an image to our network and get out a probability distribution over the classes that tells us the likely class(es) the image belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax function\n",
    "def softmax(x):\n",
    "    return torch.exp(x)/torch.sum(torch.exp(x), dim=1).view(-1,1)\n",
    "\n",
    "probabilities = softmax(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building networks with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden = nn.Linear(784, 256)\n",
    "        self.output = nn.Linear(256, 10)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "         \n",
    "        return x\n",
    "\n",
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same architetture in a cleaner way using the torch.nn.functional module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.hidden = nn.Linear(784, 256)\n",
    "        # Output layer, 10 units - one for each digit\n",
    "        self.output = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Hidden layer with sigmoid activation\n",
    "        x = F.sigmoid(self.hidden(x))\n",
    "        # Output layer with softmax activation\n",
    "        x = F.softmax(self.output(x), dim=1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Defining the layers, 128, 64, 10 units each\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        # Output layer, 10 units - one for each digit\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' Forward pass through the network, returns the output logits '''\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing weights and biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights and such are automatically initialized for you, but it's possible to customize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0155,  0.0174, -0.0056,  ..., -0.0340, -0.0355, -0.0222],\n",
      "        [-0.0040, -0.0357,  0.0069,  ..., -0.0307,  0.0221, -0.0314],\n",
      "        [ 0.0080, -0.0236,  0.0210,  ..., -0.0354,  0.0245, -0.0065],\n",
      "        ...,\n",
      "        [-0.0293,  0.0064,  0.0294,  ..., -0.0275, -0.0154, -0.0019],\n",
      "        [-0.0190,  0.0011, -0.0316,  ..., -0.0030, -0.0355,  0.0354],\n",
      "        [ 0.0175,  0.0088, -0.0012,  ..., -0.0202,  0.0020, -0.0237]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0120,  0.0247, -0.0237, -0.0140, -0.0316,  0.0273,  0.0235, -0.0230,\n",
      "         0.0151, -0.0214,  0.0305, -0.0271,  0.0096,  0.0239, -0.0084, -0.0199,\n",
      "         0.0214, -0.0285,  0.0344, -0.0066,  0.0305, -0.0092,  0.0169,  0.0063,\n",
      "        -0.0224,  0.0217, -0.0110,  0.0345, -0.0130, -0.0084, -0.0317,  0.0052,\n",
      "         0.0311,  0.0111,  0.0225,  0.0125,  0.0137, -0.0238, -0.0350,  0.0171,\n",
      "        -0.0117, -0.0230, -0.0346, -0.0111, -0.0113,  0.0306,  0.0152, -0.0068,\n",
      "        -0.0041,  0.0183, -0.0260, -0.0191, -0.0025, -0.0156, -0.0230, -0.0036,\n",
      "         0.0190,  0.0112, -0.0059,  0.0153, -0.0064, -0.0227,  0.0228, -0.0012,\n",
      "         0.0005,  0.0259,  0.0034,  0.0063,  0.0069, -0.0248, -0.0027,  0.0244,\n",
      "         0.0118,  0.0315,  0.0199,  0.0184,  0.0057,  0.0167,  0.0319,  0.0007,\n",
      "         0.0107, -0.0150, -0.0098,  0.0206, -0.0328, -0.0154, -0.0290,  0.0178,\n",
      "        -0.0110,  0.0161,  0.0135,  0.0301,  0.0227, -0.0320, -0.0295, -0.0041,\n",
      "         0.0182, -0.0303, -0.0259, -0.0168, -0.0063, -0.0285, -0.0285,  0.0101,\n",
      "        -0.0073,  0.0284, -0.0131, -0.0213,  0.0196, -0.0209,  0.0158,  0.0088,\n",
      "        -0.0116,  0.0196,  0.0183,  0.0196,  0.0137,  0.0171,  0.0183,  0.0059,\n",
      "        -0.0165,  0.0293,  0.0318, -0.0334, -0.0179,  0.0038,  0.0306, -0.0356],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.fc1.weight)\n",
    "print(model.fc1.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set biases to all zeros\n",
    "model.fc1.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0292,  0.0140,  0.0068,  ..., -0.0009,  0.0109, -0.0028],\n",
       "        [ 0.0189,  0.0049, -0.0018,  ...,  0.0048,  0.0025,  0.0035],\n",
       "        [ 0.0152,  0.0033,  0.0029,  ...,  0.0134, -0.0114,  0.0097],\n",
       "        ...,\n",
       "        [-0.0006,  0.0046, -0.0013,  ...,  0.0018,  0.0051,  0.0213],\n",
       "        [-0.0162, -0.0092,  0.0026,  ...,  0.0080, -0.0035, -0.0030],\n",
       "        [-0.0028, -0.0014,  0.0068,  ..., -0.0007,  0.0004,  0.0009]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample from random normal with standard dev = 0.01\n",
    "model.fc1.weight.data.normal_(std=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.8300e-03,  2.1343e-03,  4.5210e-03,  ...,  1.2556e-02,\n",
       "          8.4191e-03, -7.7170e-03],\n",
       "        [-1.3803e-02, -1.6898e-02, -1.6969e-02,  ...,  5.7594e-03,\n",
       "          1.9165e-02,  1.9431e-02],\n",
       "        [ 3.1676e-03,  1.5446e-03, -5.9974e-03,  ..., -1.5712e-02,\n",
       "          1.0880e-02,  1.0024e-02],\n",
       "        ...,\n",
       "        [ 1.1801e-02, -7.1896e-03, -1.7923e-03,  ..., -1.0867e-02,\n",
       "         -1.7662e-03, -3.2468e-03],\n",
       "        [-1.4617e-02,  2.2918e-03, -7.0998e-03,  ...,  8.0740e-03,\n",
       "         -1.9728e-02, -3.1923e-04],\n",
       "        [ 2.0913e-03, -2.3715e-03,  7.7373e-05,  ...,  2.2230e-02,\n",
       "          3.0970e-03,  2.0175e-02]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample from random normal with standard dev = 0.01\n",
    "model.fc1.weight.data.normal_(std=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab some data \n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Architetture of the network\n",
    "input_size = 784\n",
    "hidden_size = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "#Build a feed forward network\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_size[0]),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(hidden_size[0], hidden_size[1]),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(hidden_size[1], output_size),\n",
    "                     nn.Softmax(dim=1))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass through the network and display output\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(images.shape[0], 1, 784)\n",
    "ps = model.forward(images[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a feed-froward network\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.Softmax(dim=1))\n",
    "\n",
    "#Loss function: Negative Log Likelihood Loss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "#Get the data\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], 784)\n",
    "\n",
    "logps = model(images)\n",
    "loss = criterion(logps, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Backpropagation:None\n",
      "After Backpropagation: tensor([[-3.1699e-04, -3.1699e-04, -3.1699e-04,  ..., -3.1699e-04,\n",
      "         -3.1699e-04, -3.1699e-04],\n",
      "        [ 2.4233e-04,  2.4233e-04,  2.4233e-04,  ...,  2.4233e-04,\n",
      "          2.4233e-04,  2.4233e-04],\n",
      "        [ 9.0017e-05,  9.0017e-05,  9.0017e-05,  ...,  9.0017e-05,\n",
      "          9.0017e-05,  9.0017e-05],\n",
      "        ...,\n",
      "        [ 1.5411e-04,  1.5411e-04,  1.5411e-04,  ...,  1.5411e-04,\n",
      "          1.5411e-04,  1.5411e-04],\n",
      "        [-5.1036e-04, -5.1036e-04, -5.1036e-04,  ..., -5.1036e-04,\n",
      "         -5.1036e-04, -5.1036e-04],\n",
      "        [ 1.3148e-04,  1.3148e-04,  1.3148e-04,  ...,  1.3148e-04,\n",
      "          1.3148e-04,  1.3148e-04]])\n"
     ]
    }
   ],
   "source": [
    "#model[0] gives us the parameters for the first Linear layer\n",
    "print('Before Backpropagation:' + str(model[0].weight.grad))\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('After Backpropagation: '+ str(model[0].weight.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to use the gradient to update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights -  Parameter containing:\n",
      "tensor([[-0.0324, -0.0063, -0.0316,  ..., -0.0265, -0.0020,  0.0051],\n",
      "        [-0.0072, -0.0100,  0.0199,  ...,  0.0355,  0.0174,  0.0256],\n",
      "        [ 0.0054, -0.0211,  0.0330,  ..., -0.0315,  0.0265,  0.0215],\n",
      "        ...,\n",
      "        [-0.0195, -0.0081,  0.0065,  ...,  0.0164,  0.0235, -0.0157],\n",
      "        [ 0.0226, -0.0145,  0.0066,  ...,  0.0064,  0.0251,  0.0202],\n",
      "        [ 0.0260,  0.0083,  0.0239,  ..., -0.0185,  0.0235, -0.0209]],\n",
      "       requires_grad=True)\n",
      "Gradient -  tensor([[ 1.6906e-04,  1.6906e-04,  1.6906e-04,  ...,  1.6906e-04,\n",
      "          1.6906e-04,  1.6906e-04],\n",
      "        [-1.9482e-05, -1.9482e-05, -1.9482e-05,  ..., -1.9482e-05,\n",
      "         -1.9482e-05, -1.9482e-05],\n",
      "        [ 6.0669e-05,  6.0669e-05,  6.0669e-05,  ...,  6.0669e-05,\n",
      "          6.0669e-05,  6.0669e-05],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-1.7794e-05, -1.7794e-05, -1.7794e-05,  ..., -1.7794e-05,\n",
      "         -1.7794e-05, -1.7794e-05],\n",
      "        [-4.2487e-05, -4.2487e-05, -4.2487e-05,  ..., -4.2487e-05,\n",
      "         -4.2487e-05, -4.2487e-05]])\n"
     ]
    }
   ],
   "source": [
    "#Build a feed-froward network\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64,10),\n",
    "                      nn.Softmax(dim=1))\n",
    "\n",
    "#Loss function: Negative Log Likelihood Loss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "#Get the data\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(64, 784)\n",
    "\n",
    "#Clear the gradient, do this because gradients are accumulated\n",
    "optimizer.zero_grad()\n",
    "\n",
    "#Forward pass, then backward pass, then update weights\n",
    "output = model.forward(images)\n",
    "print('Initial weights - ', model[0].weight)\n",
    "\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "\n",
    "print('Gradient - ', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights -  Parameter containing:\n",
      "tensor([[-0.0324, -0.0063, -0.0316,  ..., -0.0265, -0.0020,  0.0051],\n",
      "        [-0.0072, -0.0100,  0.0199,  ...,  0.0355,  0.0174,  0.0256],\n",
      "        [ 0.0054, -0.0211,  0.0330,  ..., -0.0315,  0.0265,  0.0215],\n",
      "        ...,\n",
      "        [-0.0195, -0.0081,  0.0065,  ...,  0.0164,  0.0235, -0.0157],\n",
      "        [ 0.0226, -0.0145,  0.0066,  ...,  0.0064,  0.0251,  0.0202],\n",
      "        [ 0.0260,  0.0083,  0.0239,  ..., -0.0185,  0.0235, -0.0209]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#take an update step and few the new weights\n",
    "optimizer.step()\n",
    "print('Updated weights - ', model[0].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary - Training the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: -0.2729356850165802\n",
      "Training loss: -0.6426626615432788\n",
      "Training loss: -0.7630584062670848\n",
      "Training loss: -0.8001316902098625\n",
      "Training loss: -0.8121029971632113\n",
      "Training loss: -0.8190476884847002\n",
      "Training loss: -0.8238644276473568\n",
      "Training loss: -0.8274958256338196\n",
      "Training loss: -0.8301071693012709\n",
      "Training loss: -0.8328505146350942\n"
     ]
    }
   ],
   "source": [
    "x,y = next(iter(trainloader))\n",
    "\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64,10),\n",
    "                      nn.Softmax(dim=1))\n",
    "\n",
    "loss_func = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.03)\n",
    "\n",
    "epochs = 10\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for x,y in trainloader:\n",
    "        #Flattening\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        #Clear the gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Training\n",
    "        y_hat = model.forward(x)\n",
    "        loss = loss_func(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss = running_loss + loss.item()\n",
    "    \n",
    "    else:\n",
    "        print(f'Training loss: {running_loss/len(trainloader)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
